{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실제로 Gradient Descent를 이용할 때는 아래와 같이 복잡하게 사용 X\n",
    "- 이전처럼 함수나 모듈을 통해 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.FloatTensor([[.1, .2, .3],\n",
    "                            [.4, .5, .6],\n",
    "                            [.7, .8, .9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/105966480/210149672-119345e2-ce8c-48a1-beb8-e63405c783a4.png)\n",
    "- .1, .2 => 표기 실수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5931, 0.8655, 0.9750],\n",
       "        [0.9080, 0.1375, 0.1968],\n",
       "        [0.9310, 0.0709, 0.2003]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(target)\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation.\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- x가 target에 가까워질 수록, Loss값도 작아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/105966480/210149713-b257cd21-9bb4-4d1b-ad3b-922c50a62252.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3076, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 함수 로직\n",
    "    - 전체 흐름\n",
    "        - ![image](https://user-images.githubusercontent.com/105966480/210149762-73ea714d-2b9d-4ebe-b360-1e0ee709036a.png)\n",
    "    ---\n",
    "    - while loss > threshold:\n",
    "        - loss가 threshold보다 작을 경우\n",
    "    ---\n",
    "    - loss.backward()\n",
    "        - ![image](https://user-images.githubusercontent.com/105966480/210149830-498a5c6c-812a-4666-9572-abc43bd5e56b.png)\n",
    "        - loss를 x로 미분\n",
    "    ---\n",
    "    - x = x - learning_rate * x.grad\n",
    "        - ![image](https://user-images.githubusercontent.com/105966480/210149932-8edd8e98-fefc-4049-960a-7b13717e9215.png)\n",
    "    ---\n",
    "    - x.detach_()\n",
    "        - ![image](https://user-images.githubusercontent.com/105966480/210150061-03f86369-8aaa-4a73-812a-c3ef353b7895.png)\n",
    "        - 현재 detach를 완전히 이해할 수 없음 -> 차후 다뤄볼 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 1.8607e-01\n",
      "tensor([[0.4836, 0.7176, 0.8250],\n",
      "        [0.7951, 0.2180, 0.2864],\n",
      "        [0.8796, 0.2330, 0.3558]], requires_grad=True)\n",
      "2-th Loss: 1.1256e-01\n",
      "tensor([[0.3983, 0.6026, 0.7083],\n",
      "        [0.7073, 0.2807, 0.3561],\n",
      "        [0.8397, 0.3590, 0.4767]], requires_grad=True)\n",
      "3-th Loss: 6.8091e-02\n",
      "tensor([[0.3320, 0.5131, 0.6176],\n",
      "        [0.6390, 0.3294, 0.4103],\n",
      "        [0.8087, 0.4570, 0.5708]], requires_grad=True)\n",
      "4-th Loss: 4.1191e-02\n",
      "tensor([[0.2805, 0.4435, 0.5470],\n",
      "        [0.5859, 0.3673, 0.4525],\n",
      "        [0.7845, 0.5332, 0.6439]], requires_grad=True)\n",
      "5-th Loss: 2.4918e-02\n",
      "tensor([[0.2404, 0.3894, 0.4921],\n",
      "        [0.5446, 0.3968, 0.4852],\n",
      "        [0.7657, 0.5925, 0.7008]], requires_grad=True)\n",
      "6-th Loss: 1.5074e-02\n",
      "tensor([[0.2092, 0.3473, 0.4494],\n",
      "        [0.5125, 0.4197, 0.5107],\n",
      "        [0.7511, 0.6386, 0.7451]], requires_grad=True)\n",
      "7-th Loss: 9.1187e-03\n",
      "tensor([[0.1849, 0.3146, 0.4162],\n",
      "        [0.4875, 0.4376, 0.5306],\n",
      "        [0.7398, 0.6745, 0.7795]], requires_grad=True)\n",
      "8-th Loss: 5.5162e-03\n",
      "tensor([[0.1660, 0.2891, 0.3904],\n",
      "        [0.4680, 0.4514, 0.5460],\n",
      "        [0.7309, 0.7024, 0.8063]], requires_grad=True)\n",
      "9-th Loss: 3.3370e-03\n",
      "tensor([[0.1514, 0.2693, 0.3703],\n",
      "        [0.4529, 0.4622, 0.5580],\n",
      "        [0.7241, 0.7241, 0.8271]], requires_grad=True)\n",
      "10-th Loss: 2.0187e-03\n",
      "tensor([[0.1400, 0.2539, 0.3547],\n",
      "        [0.4412, 0.4706, 0.5673],\n",
      "        [0.7187, 0.7409, 0.8433]], requires_grad=True)\n",
      "11-th Loss: 1.2212e-03\n",
      "tensor([[0.1311, 0.2419, 0.3425],\n",
      "        [0.4320, 0.4772, 0.5746],\n",
      "        [0.7146, 0.7541, 0.8559]], requires_grad=True)\n",
      "12-th Loss: 7.3873e-04\n",
      "tensor([[0.1242, 0.2326, 0.3331],\n",
      "        [0.4249, 0.4822, 0.5802],\n",
      "        [0.7113, 0.7643, 0.8657]], requires_grad=True)\n",
      "13-th Loss: 4.4689e-04\n",
      "tensor([[0.1188, 0.2254, 0.3257],\n",
      "        [0.4194, 0.4862, 0.5846],\n",
      "        [0.7088, 0.7722, 0.8733]], requires_grad=True)\n",
      "14-th Loss: 2.7034e-04\n",
      "tensor([[0.1146, 0.2197, 0.3200],\n",
      "        [0.4151, 0.4893, 0.5880],\n",
      "        [0.7068, 0.7784, 0.8793]], requires_grad=True)\n",
      "15-th Loss: 1.6354e-04\n",
      "tensor([[0.1114, 0.2153, 0.3156],\n",
      "        [0.4117, 0.4916, 0.5907],\n",
      "        [0.7053, 0.7832, 0.8839]], requires_grad=True)\n",
      "16-th Loss: 9.8931e-05\n",
      "tensor([[0.1088, 0.2119, 0.3121],\n",
      "        [0.4091, 0.4935, 0.5928],\n",
      "        [0.7041, 0.7869, 0.8875]], requires_grad=True)\n",
      "17-th Loss: 5.9847e-05\n",
      "tensor([[0.1069, 0.2093, 0.3094],\n",
      "        [0.4071, 0.4949, 0.5944],\n",
      "        [0.7032, 0.7898, 0.8902]], requires_grad=True)\n",
      "18-th Loss: 3.6204e-05\n",
      "tensor([[0.1054, 0.2072, 0.3073],\n",
      "        [0.4055, 0.4961, 0.5956],\n",
      "        [0.7025, 0.7921, 0.8924]], requires_grad=True)\n",
      "19-th Loss: 2.1901e-05\n",
      "tensor([[0.1042, 0.2056, 0.3057],\n",
      "        [0.4043, 0.4969, 0.5966],\n",
      "        [0.7019, 0.7938, 0.8941]], requires_grad=True)\n",
      "20-th Loss: 1.3249e-05\n",
      "tensor([[0.1032, 0.2044, 0.3044],\n",
      "        [0.4033, 0.4976, 0.5974],\n",
      "        [0.7015, 0.7952, 0.8954]], requires_grad=True)\n",
      "21-th Loss: 8.0147e-06\n",
      "tensor([[0.1025, 0.2034, 0.3034],\n",
      "        [0.4026, 0.4981, 0.5979],\n",
      "        [0.7012, 0.7963, 0.8964]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold: # loss가 threshold보다 작을 경우\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    loss.backward() # Calculate gradients. # loss Scala값을 미분해라\n",
    "    \n",
    "    x = x - learning_rate * x.grad # 미분을 진행했기 때문에, x.grad(저장된 미분값 호출)값이 존재함\n",
    "    \n",
    "    # You don't need to aware this now.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
