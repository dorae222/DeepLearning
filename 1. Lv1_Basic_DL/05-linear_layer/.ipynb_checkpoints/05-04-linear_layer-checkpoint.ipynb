{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mini-batch로 parallel하게 연산하는 것이 아니라, 여러 개의 sample을 동시에 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Linear Layer\n",
    "\n",
    "$$\\begin{gathered}\n",
    "y=x\\cdot{W}+b, \\\\\n",
    "\\text{where }x\\in\\mathbb{R}^{N\\times{n}}\\text{, }y\\in\\mathbb{R}^{N\\times{m}}. \\\\\n",
    "\\\\\n",
    "\\text{Thus, }W\\in\\mathbb{R}^{n\\times{m}}\\text{ and }b\\in\\mathbb{R}^m.\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])\n",
    "b = torch.FloatTensor([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(W.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/105966480/210132989-42bd9ed8-622f-4457-97af-d9cf302be465.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x가 사실 몇 개의 sample을 가지고 있을지는 모른다(실행을 해보기 전까지는)\n",
    "- 위의 수식에서는 N의 값이 중요하지 않다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 1, 1],\n",
    "                       [2, 2, 2],\n",
    "                       [3, 3, 3],\n",
    "                       [4, 4, 4]])\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/105966480/210133092-b9092b86-252b-4b9b-afa7-3299e2a5127c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 14.],\n",
       "        [20., 26.],\n",
       "        [29., 38.],\n",
       "        [38., 50.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재는 계산식만 세운 것일 뿐, 학습을 시킬 수는 없음\n",
    "- w,b가 학습이 되어야 하는데, 원하는 출력을 뱉어낼 수 없음\n",
    "- 이에 따라, nn.Module을 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyToch 안에 nn.Module이라는 추상클래스 존재\n",
    "    - 이후, NN Layer는 대부분 nn.Module을 상속하여 사용\n",
    "- DNN이 하나의 큰 함수가 있을 때\n",
    "    - 각각의 서브 모듈로 구성되어 있다\n",
    "    - Layer가 10개면, Layer 하나하나가 서브 모듈이고\n",
    "    - 이들이 합쳐져 하나의 큰(합성) 함수를 이루는 구조\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.Module에서 중요한 2가지 메서드\n",
    "1. __init__\n",
    "    - forward에서 필요한 것들을 미리 선언\n",
    "2. forward\n",
    "    - 함수에서 일어나는 과정을 선언<br>\n",
    "- 우리는 앞으로 이 2가지를 오버라이드해서 사용\n",
    "- 물론 다른 메서드들도 존재하지만, 상속할 case가 잘 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyLinear Class 선언\n",
    "# 동시에 nn.Module 상속\n",
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        # __init__에서는 입력층과 출력층을 받음\n",
    "        # 위의 수식에서는\n",
    "        # input_dim = 3 = n\n",
    "        # output_dim = 2 = m\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__() # 상위 __init__ 메서드 호출\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_dim, output_dim) #(3,2)로 생성\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "    # You should override 'forward' method to implement detail.\n",
    "    # The input arguments and outputs can be designed as you wish.\n",
    "    def forward(self, x):\n",
    "        # x1,x2등 내가 원하는 arg들을 가져올 수 있음\n",
    "        # y = f(x) 선언 과정\n",
    "        \n",
    "        # |x| = (batch_size, input_dim)\n",
    "        # batch_size = 우리가 parallel하게 처리할 sample의 숫자\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is no weight parameters to learn.\n",
    "Above way can forward(or calculate) values, but it cannot be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.Module 안에 .parameters() 기능이 내장되어 있음\n",
    "    - trainable한 parameter들을 iterable하게 return\n",
    "- 위의 MyLinear Class 자체를 현재 잘못 구성했음을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct way: nn.Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.Parameter를 한 번더 wrapping해주면 됨.\n",
    "- 그래야만 nn.Module을 상속받은 MyLinear 객체에 제대로 register 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.3237e+22, 3.4152e-06],\n",
      "        [6.6468e+22, 5.2359e+22],\n",
      "        [2.6517e-09, 1.0394e+21]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([9.2755e-39, 1.0561e-38], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# W, b가 잘 register되었음을 확인 가능\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 직접 Linear 함수를 구현하는게 아니라, 내장된 nn.Linear를 가져와서 사용\n",
    "    - 매번 구현하면 힘들기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3, 2) # Linear라는 객체로 assign\n",
    "\n",
    "y = linear(x) # .forward를 호출하지 않음 -> PyTorch에서 자동적으로 호출해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7101, -0.2465],\n",
       "        [ 1.5387, -0.8626],\n",
       "        [ 2.3674, -1.4787],\n",
       "        [ 3.1961, -2.0949]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0118,  0.2891,  0.5514],\n",
      "        [-0.5112, -0.0496, -0.0554]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1186,  0.3697], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 W 파라미터\n",
    "    - ![image](https://user-images.githubusercontent.com/105966480/210133953-5941909f-eca4-41d2-be79-6dcc1ee029d4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module can contain other nn.Module's child classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module 안에 또 다른 nn.Module이 존재가 가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0266,  0.3757,  0.3403],\n",
      "        [-0.3745, -0.3462,  0.2984]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2712,  0.4359], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
